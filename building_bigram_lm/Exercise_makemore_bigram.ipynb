{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "ok6EckRziegB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "M-voh6wKTAos"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exericise 1: Building a trigram lm"
      ],
      "metadata": {
        "id": "lGBkW9adjoRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q : Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model? "
      ],
      "metadata": {
        "id": "VLtjREpxWST-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file=open('/content/drive/MyDrive/building_makemore/names.txt')"
      ],
      "metadata": {
        "id": "DdHw5JkPi3Z9"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=file.read().split('\\n')"
      ],
      "metadata": {
        "id": "2t7Hi0Mlj1ve"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNazaAHbj29F",
        "outputId": "25006ebd-44e0-4dd2-bba4-54306822234d"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a trigram lm, we will need 2 characters in the predictor used to predict the next character unlike a bigram in which we needed only one previous character"
      ],
      "metadata": {
        "id": "bGt4pmwMkjmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".em\n",
        "emm\n",
        "mma\n",
        "ma."
      ],
      "metadata": {
        "id": "o7FUj1ZIk-hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words[:1]:\n",
        "  chars=['.']+list(word)+['.']\n",
        "\n",
        "  for f,s,t in zip(chars,chars[1:],chars[2:]):\n",
        "    print(f,s,t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ8oi4Uwj_1E",
        "outputId": "62c51f22-1656-4a10-dc82-f9aa5ef06ecd"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". e m\n",
            "e m m\n",
            "m m a\n",
            "m a .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word \"emma\" specifically would contribute 4 training samples."
      ],
      "metadata": {
        "id": "rnIdu6limeQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the training set using the entire corpus and segregating the the first two characters into xs and the 3rd character into ys"
      ],
      "metadata": {
        "id": "_yVsqokNmwo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First convert characters into integers by building a stoi and itos dictionary\n",
        "\n",
        "# xs=[]\n",
        "# ys=[]\n",
        "\n",
        "# for word in words:\n",
        "#   chars=['.']+list(word)+['.']\n",
        "\n",
        "#   for f,s,t in zip(chars,chars[1:],chars[2:]):\n",
        "#     xs.append()\n",
        "#     ys.append()\n",
        "\n"
      ],
      "metadata": {
        "id": "cH4CBLLOlxv4"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, i realised that the predictor array i will have will contain characters. First we need a way to change characters into integers\n"
      ],
      "metadata": {
        "id": "3J84e0y7n3dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the dictionary- just a sample on how to proceed\n",
        "\n",
        "for i,c in enumerate(sorted(list(set(''.join(words[:2]))))):\n",
        "  print(i,c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig6PEG5roFi0",
        "outputId": "d149974b-3ad0-495e-d953-34a9430a5663"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 a\n",
            "1 e\n",
            "2 i\n",
            "3 l\n",
            "4 m\n",
            "5 o\n",
            "6 v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi={c:i+1 for i,c in enumerate(sorted(list(set(''.join(words)))))}\n",
        "stoi['.']=0\n",
        "itos={i:c for c,i in stoi.items()}\n"
      ],
      "metadata": {
        "id": "D-64uv9zomT0"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "itos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_Ls9-JhqR7p",
        "outputId": "a6e27236-28de-45e7-9853-3487ff5cb787"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'a',\n",
              " 2: 'b',\n",
              " 3: 'c',\n",
              " 4: 'd',\n",
              " 5: 'e',\n",
              " 6: 'f',\n",
              " 7: 'g',\n",
              " 8: 'h',\n",
              " 9: 'i',\n",
              " 10: 'j',\n",
              " 11: 'k',\n",
              " 12: 'l',\n",
              " 13: 'm',\n",
              " 14: 'n',\n",
              " 15: 'o',\n",
              " 16: 'p',\n",
              " 17: 'q',\n",
              " 18: 'r',\n",
              " 19: 's',\n",
              " 20: 't',\n",
              " 21: 'u',\n",
              " 22: 'v',\n",
              " 23: 'w',\n",
              " 24: 'x',\n",
              " 25: 'y',\n",
              " 26: 'z',\n",
              " 0: '.'}"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzYm91r5qi7h",
        "outputId": "d2cd67a9-dcbc-4e9c-f259-1f8b19c6d50d"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 1,\n",
              " 'b': 2,\n",
              " 'c': 3,\n",
              " 'd': 4,\n",
              " 'e': 5,\n",
              " 'f': 6,\n",
              " 'g': 7,\n",
              " 'h': 8,\n",
              " 'i': 9,\n",
              " 'j': 10,\n",
              " 'k': 11,\n",
              " 'l': 12,\n",
              " 'm': 13,\n",
              " 'n': 14,\n",
              " 'o': 15,\n",
              " 'p': 16,\n",
              " 'q': 17,\n",
              " 'r': 18,\n",
              " 's': 19,\n",
              " 't': 20,\n",
              " 'u': 21,\n",
              " 'v': 22,\n",
              " 'w': 23,\n",
              " 'x': 24,\n",
              " 'y': 25,\n",
              " 'z': 26,\n",
              " '.': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets build it using manual method first i.e using counts matrix"
      ],
      "metadata": {
        "id": "wiHw1BQnn7j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will store the counts of each trigram which occurs in our dataset\n",
        "N=torch.zeros((27,27,27),dtype=torch.int32)\n",
        "\n",
        "for word in words:\n",
        "  chars=['.']+list(word)+['.']\n",
        "\n",
        "  for f,s,t in zip(chars,chars[1:],chars[2:]):\n",
        "    ch1=stoi[f]\n",
        "    ch2=stoi[s]\n",
        "    ch3=stoi[t]\n",
        "    N[ch1,ch2,ch3]+=1\n",
        "    "
      ],
      "metadata": {
        "id": "-goNH_LvoByv"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "******"
      ],
      "metadata": {
        "id": "1DIEl_5bUxLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rough work- checks"
      ],
      "metadata": {
        "id": "Y5WAimMoU2If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This basically gives the counts of trigrams which start with '.' and 'a' with 3rd character chosen from the 27 characters\n",
        "N[0,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BctKt4kRIgN",
        "outputId": "43d1f70c-f322-46e6-eaae-810245741331"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  0, 207, 190,  31, 366,  55,  21,  17,  91, 154,  27,  75, 632, 384,\n",
              "        623,  10,  17,   9, 482, 194,  72, 152, 243,   6,  27, 173, 152],\n",
              "       dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This basically sums all counts and will be used in normalising the values of N[0,1] to give us probabilities\n",
        "N[0,1].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifP_1r6aUPdS",
        "outputId": "d34a4d22-9130-494d-c712-33faa56d3b5a"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4410)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "******"
      ],
      "metadata": {
        "id": "39rCIQ_-UzHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This basically sums all the values across the 3rd dimension. We will use this to normalise our matrix\n",
        "N.sum(2,keepdims=True).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KISeUZbRkGb",
        "outputId": "6eefe614-6b29-46a0-ccbd-e2f1a386748a"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([27, 27, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have applied smoothing because there are some cells which might have zero in them (which are impossible to occur according to the dataset)\n",
        "P=(N+1).float()\n",
        "P/=P.sum(2,keepdims=True)"
      ],
      "metadata": {
        "id": "d4tDPD3kVEU1"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for k in range(10):\n",
        "      out=[]\n",
        "      ix,iy=0,0\n",
        "\n",
        "      while True:\n",
        "          probs=P[ix,iy]\n",
        "          ix=iy\n",
        "          iy = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
        "          out.append(itos[iy])\n",
        "          \n",
        "          if iy==0:\n",
        "           break\n",
        "      print(''.join(out))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zLaFeXoWIFx",
        "outputId": "f14128c6-e8a7-46d3-b847-ae1861cb3065"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quia.\n",
            "yu.\n",
            "quinslyntien.\n",
            "nolliahi.\n",
            "ha.\n",
            "quetony.\n",
            "uri.\n",
            "tackareibaidredelingh.\n",
            "willah.\n",
            "zulm.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we have a trigram model in which if we give starting 2 characters of a word, we get the most probable next characters till the next character predicted is . This is autoregressive model"
      ],
      "metadata": {
        "id": "DXFxTbqWgDgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "******"
      ],
      "metadata": {
        "id": "mC00HgXygW2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We will now improve our model by tuning the weigths to that the words produced at more meaningful"
      ],
      "metadata": {
        "id": "GRTaeGd7gYhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logprobs=0\n",
        "count=0\n",
        "\n",
        "for w in words:\n",
        "    # Adding start and end tokens to denote start and end of a word\n",
        "    chs=['.']+list(w)+['.']\n",
        "    for c1,c2,c3 in zip(chs,chs[1:],chs[2:]):\n",
        "        ch1=stoi[c1]\n",
        "        ch2=stoi[c2]\n",
        "        ch3=stoi[c3]\n",
        "        #N[ch1,ch2]+=1\n",
        "        probs=P[ch1,ch2,ch3]\n",
        "        lp=torch.log(probs)\n",
        "        logprobs+=lp\n",
        "        count+=1\n",
        "        #print(f'{ch1}{ch2}{ch3}: {probs:.4f}: {lp:.4f}')\n",
        "print(f'{logprobs=}')\n",
        "nll=-logprobs\n",
        "# Where nll is the negative log likelihood because we want low loss to be good but log likelihood says low value of loss is bad hence negating log likelihood\n",
        "print(f'{nll=}')\n",
        "# The below is average negative log likelihood. This would be our loss function generally.\n",
        "print(f'{nll/count}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pXwElL6ghrS",
        "outputId": "caf12fa7-94ec-49a6-de49-ac028a655d06"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs=tensor(-410414.9688)\n",
            "nll=tensor(410414.9688)\n",
            "2.092747449874878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1 solution: The min loss observed in trigram model was 2.092 whereas the min loss observed in bigram model was 2.45 so trigram model improved over bigram model."
      ],
      "metadata": {
        "id": "lxoQg-6cVeSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "LWGqJSeE0Usp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use neural networks to do the same as above"
      ],
      "metadata": {
        "id": "mAdxBvjF0WUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W=torch.randn((27*2,27),generator=g,requires_grad=True)"
      ],
      "metadata": {
        "id": "gVYEq4In0bJa"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw4bVqZMtuV8",
        "outputId": "49a64599-e460-41d0-d872-9cce5c66f9c2"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([54, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OztbO4equ0A5",
        "outputId": "808444b7-eb5f-4bc9-a990-38d7b4a2a72b"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating training set i.e having predictors and target in xs and ys\n",
        "\n",
        "xs=[]\n",
        "ys=[]\n",
        "\n",
        "for word in words[:1]:\n",
        "  chars=['.']+list(word)+['.']\n",
        "\n",
        "  for f,s,t in zip(chars,chars[1:],chars[2:]):\n",
        "    ch1=stoi[f]\n",
        "    ch2=stoi[s]\n",
        "    ch3=stoi[t]\n",
        "    xs.append([ch1,ch2])\n",
        "    ys.append(ch3)"
      ],
      "metadata": {
        "id": "nmn5UpMz0rzU"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs=torch.tensor(xs)\n",
        "ys=torch.tensor(ys)"
      ],
      "metadata": {
        "id": "raluOpZZtemX"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1nXBgY6vAQD",
        "outputId": "938c99cd-3f9c-422c-b166-a0768df64779"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  5],\n",
              "        [ 5, 13],\n",
              "        [13, 13],\n",
              "        [13,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "new_xs=F.one_hot(xs,num_classes=27)"
      ],
      "metadata": {
        "id": "qZBWaM9h23DT"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flattens the inside lists and it essentially concatenates one_hot representations for both input values of a trigram\n",
        "new_xs=(new_xs.reshape(4,54)).float()"
      ],
      "metadata": {
        "id": "1YZNja_LvJjT"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Where each row will give us the activations of 27 neurons for each example as in the 0th row is giving us activations of each of the 27 neurons and the one which gives the heighest prob will be chosen.\n",
        "(new_xs@W)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXOtMa0xzHzG",
        "outputId": "78729292-61b0-4c92-85ce-a4a99b6be60d"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.5352,  0.2418, -0.2616, -1.0364,  0.4752, -0.3721, -0.2249, -0.4349,\n",
              "        -0.6773,  1.1168,  1.6238,  1.3033,  0.3456,  0.5469,  2.9616,  1.7805,\n",
              "         0.8366, -0.8746,  2.0334,  0.3803,  0.6542, -0.8347, -1.6931,  1.4028,\n",
              "        -1.6675,  0.6432,  1.0764], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "logits = new_xs @ W # predict log-counts\n",
        "counts = logits.exp() # counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "-probs[torch.arange(4), ys].log().mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkbQTfnR1haO",
        "outputId": "44a65d70-d361-483e-8cb9-444402992bc0"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.0953, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs[torch.arange(4), ys]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKJcWsXE8G4s",
        "outputId": "f74ccdb1-7831-4915-f29d-142c580e65d4"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0240, 0.0074, 0.0417, 0.0104], grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(4)"
      ],
      "metadata": {
        "id": "fobPxsoj8KVw",
        "outputId": "b1b8664d-c12d-4fbc-b12c-2e4d94a07faf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(counts/(counts.sum(1,keepdims=True)))[0].sum()#.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr3huXjj4Hs6",
        "outputId": "4ebf76fc-c63b-41db-a468-b7ceb5e5e46f"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GuBJTSx3jN7",
        "outputId": "97700c3d-ad5a-45c3-ac1f-b8f0cd2cbfb0"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7078,  1.2735,  0.7699,  0.3547,  1.6084,  0.6893,  0.7986,  0.6473,\n",
              "          0.5080,  3.0550,  5.0724,  3.6814,  1.4128,  1.7279, 19.3293,  5.9326,\n",
              "          2.3085,  0.4170,  7.6398,  1.4628,  1.9236,  0.4340,  0.1840,  4.0667,\n",
              "          0.1887,  1.9026,  2.9342],\n",
              "        [ 2.2475,  4.8279,  1.1039,  6.7569,  6.9147,  0.3263,  1.0807,  0.5787,\n",
              "          0.9320,  0.4274, 10.9643,  8.6845,  0.5825,  0.7380,  0.0906,  0.8456,\n",
              "          1.3473,  0.4381,  6.5185,  0.4579,  3.2748,  6.3326,  7.2101,  0.0643,\n",
              "          0.4045,  0.7889, 26.1892],\n",
              "        [ 1.7007,  3.1414,  1.5148,  3.0331,  1.6480,  0.1593,  2.1359,  0.3030,\n",
              "          8.2787,  0.1639,  0.8219, 21.0960,  0.4684,  0.7675,  0.0864,  0.2122,\n",
              "          1.9079,  0.8272,  3.2527,  0.0873,  0.9850,  1.1654,  1.3427,  0.3423,\n",
              "          2.9067, 12.0535,  4.9325],\n",
              "        [ 0.7626,  5.7160,  7.3716,  1.3682, 14.5994,  3.2246,  1.3039,  0.4050,\n",
              "         12.8516,  1.3124,  0.0477,  4.7639,  0.1943,  1.0369,  0.8931,  2.3045,\n",
              "          1.1014,  0.4869,  1.4371,  0.0405,  1.6825,  0.0817,  1.4474,  1.7256,\n",
              "          4.7184,  1.9909,  0.2931]], grad_fn=<ExpBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyDKZ5Nn2Al6",
        "outputId": "72b489e8-0813-4098-e1d7-e2dd66f88e7d"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****"
      ],
      "metadata": {
        "id": "jyM2ueIW01NA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Doing above for the entire dataset"
      ],
      "metadata": {
        "id": "eWozrLnh050K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating training set i.e having predictors and target in xs and ys\n",
        "\n",
        "xs=[]\n",
        "ys=[]\n",
        "\n",
        "for word in words:\n",
        "  chars=['.']+list(word)+['.']\n",
        "\n",
        "  for f,s,t in zip(chars,chars[1:],chars[2:]):\n",
        "    ch1=stoi[f]\n",
        "    ch2=stoi[s]\n",
        "    ch3=stoi[t]\n",
        "    xs.append([ch1,ch2])\n",
        "    ys.append(ch3)\n",
        "\n",
        "xs=torch.tensor(xs)\n",
        "ys=torch.tensor(ys)"
      ],
      "metadata": {
        "id": "V7eLbOsU0-4T"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs.nelement()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD4693_-Lq09",
        "outputId": "053e766b-97cd-40de-e5e2-78279b5e5852"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "392226"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(xs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJr4zcrhMQiy",
        "outputId": "9c7c0824-50be-4517-d8a6-cd087882a0a5"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W=torch.randn((27*2,27),requires_grad=True)"
      ],
      "metadata": {
        "id": "19wejKcX1McP"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs=torch.tensor(xs)"
      ],
      "metadata": {
        "id": "F49i6yZE2H_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a457139-b3e5-45ab-d167-a9c292c792f3"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-175-4665cfed8f51>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  xs=torch.tensor(xs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "new_xs=F.one_hot(xs, num_classes=27)"
      ],
      "metadata": {
        "id": "1VEXm45IAaKM"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_xs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKUvPA0AAlle",
        "outputId": "eafa9ab5-a2da-4873-c884-97bec2c145a1"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([196113, 2, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_xs=new_xs.reshape(196113,-1).float()"
      ],
      "metadata": {
        "id": "iQEzDoN4AvFU"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ys.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLVOUqSrDbsN",
        "outputId": "c6a92d8b-0f9e-4e4b-b03e-a9ce3fa85495"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([196113])"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We get activations for each of the trigram from each of the 27 neurons\n",
        "(new_xs@W).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf0e9VBVA-7t",
        "outputId": "e96ecf25-7c9e-4ea2-9ffb-22b251950965"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([196113, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the loss and doing backprop\n",
        "log_counts=(new_xs@W) # This is the log counts\n",
        "counts=log_counts.exp() # This is the counts\n",
        "probs=counts/counts.sum(1,keepdims=True) # This is probability\n",
        "nll=-probs[torch.arange(196113),ys].log().mean()"
      ],
      "metadata": {
        "id": "NRJGVhuoCM2w"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nll\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9iH1CSsDkSh",
        "outputId": "23d9a5d4-1021-4cac-801e-6775d7118b98"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.2833, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.grad=None\n",
        "nll.backward()\n",
        "W.data += -0.1 * W.grad"
      ],
      "metadata": {
        "id": "zYYLRI8rDuNZ"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "uIJS8MwoJRPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Doing the optimisation for some iteration and putting it all together"
      ],
      "metadata": {
        "id": "hDbcmRNiJTyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs=[]\n",
        "ys=[]\n",
        "\n",
        "for word in words:\n",
        "  chars=['.']+list(word)+['.']\n",
        "\n",
        "  for f,s,t in zip(chars,chars[1:],chars[2:]):\n",
        "    ch1=stoi[f]\n",
        "    ch2=stoi[s]\n",
        "    ch3=stoi[t]\n",
        "    xs.append([ch1,ch2])\n",
        "    ys.append(ch3)\n",
        "    \n",
        "\n",
        "xs=torch.tensor(xs)\n",
        "ys=torch.tensor(ys)\n",
        "\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "71CDSL6JJTEl"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfgnxML0ODuC",
        "outputId": "3094427a-77c4-4dab-e52d-e2a526c3a4ea"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([196113, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.one_hot(xs,num_classes=27).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjZ06Z2CNjeR",
        "outputId": "042d9bf4-f6e3-47d2-b71c-4520299fc574"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([196113, 2, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for k in range(200):\n",
        "  \n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  xenc=xenc.reshape(196113,-1)\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "  \n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "  \n",
        "  # update\n",
        "  W.data += -55 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5S3ycn0K-Ue",
        "outputId": "19f88d23-f3f1-4449-8396-4640f0885ee0"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.1959710121154785\n",
            "3.3125147819519043\n",
            "3.0034217834472656\n",
            "2.840965747833252\n",
            "2.7417449951171875\n",
            "2.672031879425049\n",
            "2.61826753616333\n",
            "2.5762722492218018\n",
            "2.5418875217437744\n",
            "2.5140926837921143\n",
            "2.490588665008545\n",
            "2.4712564945220947\n",
            "2.4543848037719727\n",
            "2.4403278827667236\n",
            "2.427609443664551\n",
            "2.4169716835021973\n",
            "2.4069724082946777\n",
            "2.3986923694610596\n",
            "2.3905720710754395\n",
            "2.3840174674987793\n",
            "2.3772475719451904\n",
            "2.37200927734375\n",
            "2.3662304878234863\n",
            "2.362025022506714\n",
            "2.356982946395874\n",
            "2.3536040782928467\n",
            "2.349114179611206\n",
            "2.346407651901245\n",
            "2.3423373699188232\n",
            "2.340183734893799\n",
            "2.3364367485046387\n",
            "2.3347418308258057\n",
            "2.3312506675720215\n",
            "2.3299386501312256\n",
            "2.3266549110412598\n",
            "2.325665235519409\n",
            "2.322554588317871\n",
            "2.3218369483947754\n",
            "2.318873405456543\n",
            "2.318387746810913\n",
            "2.315552234649658\n",
            "2.3152644634246826\n",
            "2.3125417232513428\n",
            "2.3124241828918457\n",
            "2.3098015785217285\n",
            "2.309831380844116\n",
            "2.307297706604004\n",
            "2.307455062866211\n",
            "2.305001974105835\n",
            "2.3052709102630615\n",
            "2.302889823913574\n",
            "2.3032562732696533\n",
            "2.3009395599365234\n",
            "2.3013925552368164\n",
            "2.2991344928741455\n",
            "2.2996633052825928\n",
            "2.297459125518799\n",
            "2.298055410385132\n",
            "2.2959001064300537\n",
            "2.296555995941162\n",
            "2.294445276260376\n",
            "2.2951548099517822\n",
            "2.2930848598480225\n",
            "2.2938425540924072\n",
            "2.2918100357055664\n",
            "2.2926111221313477\n",
            "2.2906134128570557\n",
            "2.2914535999298096\n",
            "2.289487361907959\n",
            "2.290363073348999\n",
            "2.288426637649536\n",
            "2.289334774017334\n",
            "2.2874252796173096\n",
            "2.288363218307495\n",
            "2.2864789962768555\n",
            "2.2874436378479004\n",
            "2.285583257675171\n",
            "2.2865726947784424\n",
            "2.284734010696411\n",
            "2.2857463359832764\n",
            "2.283928394317627\n",
            "2.284961700439453\n",
            "2.2831625938415527\n",
            "2.2842154502868652\n",
            "2.2824347019195557\n",
            "2.2835052013397217\n",
            "2.281741142272949\n",
            "2.2828288078308105\n",
            "2.2810800075531006\n",
            "2.2821831703186035\n",
            "2.280449390411377\n",
            "2.281567096710205\n",
            "2.2798471450805664\n",
            "2.2809786796569824\n",
            "2.2792716026306152\n",
            "2.2804160118103027\n",
            "2.278721332550049\n",
            "2.279877185821533\n",
            "2.278193950653076\n",
            "2.2793614864349365\n",
            "2.277689218521118\n",
            "2.278867244720459\n",
            "2.277205228805542\n",
            "2.278393268585205\n",
            "2.276740312576294\n",
            "2.2779386043548584\n",
            "2.276294708251953\n",
            "2.2775018215179443\n",
            "2.275866746902466\n",
            "2.2770817279815674\n",
            "2.2754552364349365\n",
            "2.2766780853271484\n",
            "2.275059461593628\n",
            "2.27629017829895\n",
            "2.2746784687042236\n",
            "2.275916576385498\n",
            "2.2743117809295654\n",
            "2.275557041168213\n",
            "2.273958921432495\n",
            "2.275209903717041\n",
            "2.273618459701538\n",
            "2.2748758792877197\n",
            "2.2732903957366943\n",
            "2.2745537757873535\n",
            "2.2729737758636475\n",
            "2.274242877960205\n",
            "2.2726683616638184\n",
            "2.2739429473876953\n",
            "2.2723731994628906\n",
            "2.273653030395508\n",
            "2.2720882892608643\n",
            "2.2733728885650635\n",
            "2.271813154220581\n",
            "2.273102045059204\n",
            "2.2715466022491455\n",
            "2.2728400230407715\n",
            "2.271289348602295\n",
            "2.2725870609283447\n",
            "2.2710399627685547\n",
            "2.272341728210449\n",
            "2.270799160003662\n",
            "2.272104501724243\n",
            "2.2705655097961426\n",
            "2.27187442779541\n",
            "2.270339250564575\n",
            "2.2716517448425293\n",
            "2.270120143890381\n",
            "2.2714364528656006\n",
            "2.2699077129364014\n",
            "2.2712268829345703\n",
            "2.2697014808654785\n",
            "2.271023750305176\n",
            "2.2695016860961914\n",
            "2.270826816558838\n",
            "2.269307851791382\n",
            "2.2706353664398193\n",
            "2.2691192626953125\n",
            "2.2704505920410156\n",
            "2.2689366340637207\n",
            "2.2702701091766357\n",
            "2.268758773803711\n",
            "2.270094871520996\n",
            "2.2685863971710205\n",
            "2.2699248790740967\n",
            "2.268418312072754\n",
            "2.269759178161621\n",
            "2.2682557106018066\n",
            "2.269598960876465\n",
            "2.268097162246704\n",
            "2.269442319869995\n",
            "2.2679429054260254\n",
            "2.26928973197937\n",
            "2.2677929401397705\n",
            "2.2691421508789062\n",
            "2.2676470279693604\n",
            "2.2689976692199707\n",
            "2.2675046920776367\n",
            "2.268857479095459\n",
            "2.267366409301758\n",
            "2.2687206268310547\n",
            "2.2672317028045654\n",
            "2.268587827682495\n",
            "2.2671003341674805\n",
            "2.2684578895568848\n",
            "2.266972303390503\n",
            "2.26833176612854\n",
            "2.266847848892212\n",
            "2.2682085037231445\n",
            "2.26672625541687\n",
            "2.2680883407592773\n",
            "2.2666077613830566\n",
            "2.2679715156555176\n",
            "2.2664921283721924\n",
            "2.267857074737549\n",
            "2.2663793563842773\n",
            "2.2677454948425293\n",
            "2.2662692070007324\n",
            "2.267637014389038\n",
            "2.2661619186401367\n",
            "2.267530918121338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W.dtype"
      ],
      "metadata": {
        "id": "YEUV_MOCBkxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c309654-7208-4c25-8929-42195a430f9e"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.one_hot(torch.tensor([0,0]),num_classes=27).reshape(1,-1).shape"
      ],
      "metadata": {
        "id": "3BMywikVR0IV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad647e0-e05b-4346-b379-764778f7360a"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 54])"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, sample from the 'neural net' model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "  \n",
        "  out = []\n",
        "  ix = 0\n",
        "  iy= random.randint(0,26)\n",
        "  while True:\n",
        "    \n",
        "    \n",
        "    xenc = F.one_hot(torch.tensor([ix,iy]), num_classes=27).float()\n",
        "    xenc=xenc.reshape(1,-1)\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    # ----------\n",
        "    ix=iy\n",
        "    iy = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[iy])\n",
        "    if iy == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "id": "xqdMh3yzRvDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055e2d3d-1b4a-45be-9d94-ee11517a0a97"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rpia.\n",
            "ruanileyloryecn.\n",
            "arillais.\n",
            "aa.\n",
            "eelyeny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The min loss which we observed using nn was 2.26 while as the min possible loss which we observed using the counts method is 2.092."
      ],
      "metadata": {
        "id": "NwG-jKDzVDnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "hU6zyEH2WrHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2: Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
      ],
      "metadata": {
        "id": "kOPIxoKrWgqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First task is to split the dataset into train, validation and test set\n",
        "# train on bigram model\n",
        "# train on trigram model\n",
        "# evaluate on dev and test split\n",
        "\n",
        "# bigram first\n",
        "\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qNsSABh_WqS7"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "h_xnWdG4XOPo"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainval_xs,test_xs,trainval_ys,test_ys=train_test_split(xs,ys,random_state=42,test_size=0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "2cWSHdl-XQKY"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and validation data\n",
        "len(trainval_xs),len(trainval_ys)"
      ],
      "metadata": {
        "id": "-l8zEH8GYYFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fd7b26-49e1-417a-83fd-65e4b552a17c"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(205331, 205331)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test data\n",
        "len(test_xs),len(test_ys)"
      ],
      "metadata": {
        "id": "Hd__D3QjYcF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fc0162-74a6-4732-f639-62da24a29c05"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22815, 22815)"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into train and validation data\n",
        "\n",
        "xs_train,xs_val,ys_train,ys_val=train_test_split(trainval_xs,trainval_ys,random_state=42,test_size=0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "GeTs4FM5YlRc"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs_val.shape"
      ],
      "metadata": {
        "id": "pkPoiuqAoHcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ecff40f-7bf0-4dec-b342-720d3de559d6"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20534])"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "9b-clGEhcscS"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for k in range(100):\n",
        "  \n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs_train, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(ys_train[0]), ys_train].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "  \n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "  \n",
        "  # update\n",
        "  W.data += -13 * W.grad"
      ],
      "metadata": {
        "id": "0AaKfZuEcyp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd052ec0-cdd6-43e4-9f72-bc53d5f5bc5f"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7934956550598145\n",
            "3.0204880237579346\n",
            "2.928511381149292\n",
            "2.886056900024414\n",
            "2.8667685985565186\n",
            "2.8571183681488037\n",
            "2.8504347801208496\n",
            "2.846463918685913\n",
            "2.8434154987335205\n",
            "2.841400146484375\n",
            "2.839768648147583\n",
            "2.8386080265045166\n",
            "2.8376553058624268\n",
            "2.836942195892334\n",
            "2.836354970932007\n",
            "2.8359110355377197\n",
            "2.8355348110198975\n",
            "2.835238218307495\n",
            "2.8349928855895996\n",
            "2.8347935676574707\n",
            "2.834624767303467\n",
            "2.8344855308532715\n",
            "2.8343665599823\n",
            "2.834265947341919\n",
            "2.834181547164917\n",
            "2.8341081142425537\n",
            "2.834043025970459\n",
            "2.83398699760437\n",
            "2.833937883377075\n",
            "2.8338944911956787\n",
            "2.833855628967285\n",
            "2.8338215351104736\n",
            "2.8337903022766113\n",
            "2.833761692047119\n",
            "2.8337364196777344\n",
            "2.8337132930755615\n",
            "2.833691358566284\n",
            "2.8336706161499023\n",
            "2.8336522579193115\n",
            "2.833635091781616\n",
            "2.833618402481079\n",
            "2.8336033821105957\n",
            "2.8335886001586914\n",
            "2.8335747718811035\n",
            "2.833561897277832\n",
            "2.8335490226745605\n",
            "2.8335373401641846\n",
            "2.8335254192352295\n",
            "2.833514451980591\n",
            "2.833503484725952\n",
            "2.83349347114563\n",
            "2.8334832191467285\n",
            "2.8334734439849854\n",
            "2.8334639072418213\n",
            "2.8334550857543945\n",
            "2.8334455490112305\n",
            "2.833436965942383\n",
            "2.833428144454956\n",
            "2.8334197998046875\n",
            "2.83341121673584\n",
            "2.8334028720855713\n",
            "2.833395004272461\n",
            "2.8333868980407715\n",
            "2.833378791809082\n",
            "2.83337140083313\n",
            "2.8333640098571777\n",
            "2.8333561420440674\n",
            "2.833348512649536\n",
            "2.833341121673584\n",
            "2.833333969116211\n",
            "2.833326816558838\n",
            "2.8333194255828857\n",
            "2.833312511444092\n",
            "2.8333051204681396\n",
            "2.8332982063293457\n",
            "2.8332910537719727\n",
            "2.833284378051758\n",
            "2.8332772254943848\n",
            "2.833270311355591\n",
            "2.833263397216797\n",
            "2.833256483078003\n",
            "2.8332502841949463\n",
            "2.8332433700561523\n",
            "2.8332366943359375\n",
            "2.8332300186157227\n",
            "2.8332231044769287\n",
            "2.833216905593872\n",
            "2.8332102298736572\n",
            "2.8332037925720215\n",
            "2.8331971168518066\n",
            "2.8331902027130127\n",
            "2.833183765411377\n",
            "2.8331775665283203\n",
            "2.8331713676452637\n",
            "2.833164930343628\n",
            "2.833158493041992\n",
            "2.8331518173217773\n",
            "2.8331456184387207\n",
            "2.833139181137085\n",
            "2.833132743835449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is the minimum loss we could find by changing the learning rate"
      ],
      "metadata": {
        "id": "j11F9v0ofQV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluating on the validation and test set\n",
        "\n",
        "def evaluate_model(weight,x,yval):\n",
        "  with torch.no_grad():\n",
        "    xenc = F.one_hot(x, num_classes=27).float() \n",
        "    if len(list(xenc.shape))==3:\n",
        "      xenc=xenc.reshape(x.shape[0],-1)\n",
        "    logits = xenc @ weight\n",
        "    counts = logits.exp() \n",
        "    probs = counts / counts.sum(1, keepdims=True) \n",
        "    loss = -probs[torch.arange(yval.shape[0]), yval].log().mean()\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "3SsE6fokd2fL"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss=evaluate_model(W,xs_val,ys_val)"
      ],
      "metadata": {
        "id": "OZv3LCYAmQLp"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss"
      ],
      "metadata": {
        "id": "AkPt6OX5oRy3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653efa6c-461c-42b4-f811-4cac325623c4"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.6322)"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss=evaluate_model(W,test_xs,test_ys)"
      ],
      "metadata": {
        "id": "lMRA40fDmcgp"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss"
      ],
      "metadata": {
        "id": "MZkPUPnBKAVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7527bf5f-6c32-4b10-93d8-d1fa77651dd8"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.6252)"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The test loss and validation loss for bigram are nearly comparable"
      ],
      "metadata": {
        "id": "PvTp4LuKKI30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Doing the same for trigram"
      ],
      "metadata": {
        "id": "0xteA1W7UkQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2,ch3 in zip(chs, chs[1:],chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append([ix1,ix2])\n",
        "    ys.append(ix3)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ],
      "metadata": {
        "id": "wBndFPnZnO5S"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into (train+val) and test set\n",
        "trainval_xs_tri,test_xs_tri,trainval_ys_tri,test_ys_tri=train_test_split(xs,ys,random_state=42,test_size=0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "rM57tpwlVS7H"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into train and validation dataset\n",
        "\n",
        "xs_train_tri,xs_val_tri,ys_train_tri,ys_val_tri=train_test_split(trainval_xs_tri,trainval_ys_tri,random_state=42,test_size=0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "MsXOzsnTVyld"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "TA-3vCOWWIQl"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs_train_tri.shape[0]"
      ],
      "metadata": {
        "id": "pHzB13b4XdR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ca1df1-1fdf-4118-f51c-5e683d9be0c1"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158850"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for k in range(300):\n",
        "  \n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs_train_tri, num_classes=27).float() # input to the network: one-hot encoding\n",
        "  xenc=xenc.reshape(xs_train_tri.shape[0],-1)\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(ys_train_tri.shape[0]), ys_train_tri].log().mean() + 0.01*(W**2).mean()\n",
        "  print(loss.item())\n",
        "  \n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "  \n",
        "  # update\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "id": "4j3cSHfFWpxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02eba925-bdef-48fb-f349-e06a53641b6d"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.19407320022583\n",
            "3.364851236343384\n",
            "3.0501224994659424\n",
            "2.8792054653167725\n",
            "2.7746260166168213\n",
            "2.7020156383514404\n",
            "2.646439790725708\n",
            "2.602390766143799\n",
            "2.566504716873169\n",
            "2.5368309020996094\n",
            "2.5119502544403076\n",
            "2.490886688232422\n",
            "2.4728596210479736\n",
            "2.457282781600952\n",
            "2.443681001663208\n",
            "2.4316964149475098\n",
            "2.4210433959960938\n",
            "2.411505699157715\n",
            "2.4029104709625244\n",
            "2.3951237201690674\n",
            "2.3880343437194824\n",
            "2.381554365158081\n",
            "2.3756086826324463\n",
            "2.3701348304748535\n",
            "2.365079879760742\n",
            "2.360398530960083\n",
            "2.3560516834259033\n",
            "2.352006673812866\n",
            "2.3482322692871094\n",
            "2.3447046279907227\n",
            "2.341400623321533\n",
            "2.3383002281188965\n",
            "2.335386037826538\n",
            "2.3326430320739746\n",
            "2.33005690574646\n",
            "2.3276150226593018\n",
            "2.3253061771392822\n",
            "2.323120355606079\n",
            "2.3210487365722656\n",
            "2.3190829753875732\n",
            "2.3172152042388916\n",
            "2.315438985824585\n",
            "2.3137478828430176\n",
            "2.31213641166687\n",
            "2.310598850250244\n",
            "2.3091306686401367\n",
            "2.307727575302124\n",
            "2.3063852787017822\n",
            "2.3050999641418457\n",
            "2.3038687705993652\n",
            "2.302687406539917\n",
            "2.301553726196289\n",
            "2.300464630126953\n",
            "2.299417734146118\n",
            "2.2984108924865723\n",
            "2.2974414825439453\n",
            "2.2965073585510254\n",
            "2.295607328414917\n",
            "2.294739007949829\n",
            "2.293900966644287\n",
            "2.2930915355682373\n",
            "2.292309284210205\n",
            "2.291552782058716\n",
            "2.290821075439453\n",
            "2.2901129722595215\n",
            "2.2894270420074463\n",
            "2.288762331008911\n",
            "2.2881181240081787\n",
            "2.2874932289123535\n",
            "2.286886692047119\n",
            "2.2862980365753174\n",
            "2.285726308822632\n",
            "2.285170555114746\n",
            "2.2846312522888184\n",
            "2.284106492996216\n",
            "2.283595561981201\n",
            "2.2830986976623535\n",
            "2.2826154232025146\n",
            "2.282144069671631\n",
            "2.2816853523254395\n",
            "2.281238317489624\n",
            "2.2808024883270264\n",
            "2.2803776264190674\n",
            "2.279963493347168\n",
            "2.2795591354370117\n",
            "2.2791643142700195\n",
            "2.2787795066833496\n",
            "2.2784035205841064\n",
            "2.27803635597229\n",
            "2.2776780128479004\n",
            "2.277327299118042\n",
            "2.276984453201294\n",
            "2.2766494750976562\n",
            "2.27632212638855\n",
            "2.2760016918182373\n",
            "2.2756881713867188\n",
            "2.2753818035125732\n",
            "2.2750813961029053\n",
            "2.274787664413452\n",
            "2.2745001316070557\n",
            "2.2742185592651367\n",
            "2.273942708969116\n",
            "2.2736728191375732\n",
            "2.2734079360961914\n",
            "2.27314829826355\n",
            "2.2728946208953857\n",
            "2.2726449966430664\n",
            "2.2724006175994873\n",
            "2.2721612453460693\n",
            "2.271926164627075\n",
            "2.271695852279663\n",
            "2.271469831466675\n",
            "2.271247625350952\n",
            "2.2710299491882324\n",
            "2.2708163261413574\n",
            "2.270606517791748\n",
            "2.2704005241394043\n",
            "2.2701988220214844\n",
            "2.2699999809265137\n",
            "2.2698051929473877\n",
            "2.2696139812469482\n",
            "2.269425630569458\n",
            "2.2692408561706543\n",
            "2.269059181213379\n",
            "2.268880844116211\n",
            "2.2687058448791504\n",
            "2.268533706665039\n",
            "2.268364429473877\n",
            "2.268197774887085\n",
            "2.2680342197418213\n",
            "2.2678732872009277\n",
            "2.267714738845825\n",
            "2.26755952835083\n",
            "2.267406702041626\n",
            "2.2672557830810547\n",
            "2.2671077251434326\n",
            "2.2669620513916016\n",
            "2.2668182849884033\n",
            "2.2666773796081543\n",
            "2.266538619995117\n",
            "2.266401767730713\n",
            "2.2662672996520996\n",
            "2.2661352157592773\n",
            "2.266004800796509\n",
            "2.265876054763794\n",
            "2.265749931335449\n",
            "2.265625476837158\n",
            "2.2655029296875\n",
            "2.2653822898864746\n",
            "2.265263795852661\n",
            "2.2651467323303223\n",
            "2.265030860900879\n",
            "2.2649173736572266\n",
            "2.264805316925049\n",
            "2.264695167541504\n",
            "2.2645864486694336\n",
            "2.264479398727417\n",
            "2.264373540878296\n",
            "2.2642695903778076\n",
            "2.264167308807373\n",
            "2.264066219329834\n",
            "2.2639663219451904\n",
            "2.2638678550720215\n",
            "2.263770818710327\n",
            "2.2636752128601074\n",
            "2.2635812759399414\n",
            "2.263488292694092\n",
            "2.263396739959717\n",
            "2.263306140899658\n",
            "2.2632172107696533\n",
            "2.2631289958953857\n",
            "2.263042449951172\n",
            "2.2629570960998535\n",
            "2.2628724575042725\n",
            "2.262789011001587\n",
            "2.262706756591797\n",
            "2.2626256942749023\n",
            "2.2625458240509033\n",
            "2.2624664306640625\n",
            "2.2623884677886963\n",
            "2.2623112201690674\n",
            "2.262235403060913\n",
            "2.262160539627075\n",
            "2.2620863914489746\n",
            "2.2620131969451904\n",
            "2.2619409561157227\n",
            "2.2618696689605713\n",
            "2.2617990970611572\n",
            "2.2617297172546387\n",
            "2.2616610527038574\n",
            "2.2615931034088135\n",
            "2.261526107788086\n",
            "2.261460542678833\n",
            "2.26139497756958\n",
            "2.2613303661346436\n",
            "2.2612664699554443\n",
            "2.2612032890319824\n",
            "2.261141300201416\n",
            "2.2610795497894287\n",
            "2.261019229888916\n",
            "2.2609589099884033\n",
            "2.260899782180786\n",
            "2.260841131210327\n",
            "2.2607831954956055\n",
            "2.260725975036621\n",
            "2.260669231414795\n",
            "2.260613203048706\n",
            "2.2605581283569336\n",
            "2.2605037689208984\n",
            "2.2604494094848633\n",
            "2.2603964805603027\n",
            "2.2603437900543213\n",
            "2.26029109954834\n",
            "2.260239601135254\n",
            "2.2601888179779053\n",
            "2.260138511657715\n",
            "2.2600884437561035\n",
            "2.2600388526916504\n",
            "2.2599902153015137\n",
            "2.2599422931671143\n",
            "2.259894371032715\n",
            "2.259847402572632\n",
            "2.259800672531128\n",
            "2.2597546577453613\n",
            "2.259708881378174\n",
            "2.2596638202667236\n",
            "2.2596194744110107\n",
            "2.259575128555298\n",
            "2.2595314979553223\n",
            "2.259488344192505\n",
            "2.2594456672668457\n",
            "2.2594032287597656\n",
            "2.259361743927002\n",
            "2.2593202590942383\n",
            "2.259279251098633\n",
            "2.2592387199401855\n",
            "2.2591986656188965\n",
            "2.2591588497161865\n",
            "2.259119749069214\n",
            "2.2590811252593994\n",
            "2.259042501449585\n",
            "2.259004831314087\n",
            "2.2589669227600098\n",
            "2.258929491043091\n",
            "2.2588930130004883\n",
            "2.2588562965393066\n",
            "2.2588202953338623\n",
            "2.258784532546997\n",
            "2.25874924659729\n",
            "2.258713960647583\n",
            "2.258679151535034\n",
            "2.2586450576782227\n",
            "2.258610963821411\n",
            "2.258577585220337\n",
            "2.2585442066192627\n",
            "2.2585113048553467\n",
            "2.2584786415100098\n",
            "2.258446455001831\n",
            "2.2584142684936523\n",
            "2.258382558822632\n",
            "2.2583510875701904\n",
            "2.2583200931549072\n",
            "2.2582895755767822\n",
            "2.2582592964172363\n",
            "2.2582290172576904\n",
            "2.2581992149353027\n",
            "2.258169412612915\n",
            "2.2581403255462646\n",
            "2.2581112384796143\n",
            "2.258082389831543\n",
            "2.258054256439209\n",
            "2.258026123046875\n",
            "2.257997989654541\n",
            "2.2579705715179443\n",
            "2.2579431533813477\n",
            "2.25791597366333\n",
            "2.2578892707824707\n",
            "2.2578623294830322\n",
            "2.257835865020752\n",
            "2.25780987739563\n",
            "2.257783889770508\n",
            "2.257758378982544\n",
            "2.2577333450317383\n",
            "2.2577078342437744\n",
            "2.257683038711548\n",
            "2.2576584815979004\n",
            "2.257633924484253\n",
            "2.2576096057891846\n",
            "2.2575857639312744\n",
            "2.2575619220733643\n",
            "2.257538318634033\n",
            "2.2575151920318604\n",
            "2.2574915885925293\n",
            "2.2574689388275146\n",
            "2.257446050643921\n",
            "2.2574236392974854\n",
            "2.25740122795105\n",
            "2.2573792934417725\n",
            "2.257357358932495\n",
            "2.257335662841797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss_tri=evaluate_model(W,xs_val_tri,ys_val_tri)\n",
        "test_loss_tri=evaluate_model(W,test_xs_tri,test_ys_tri)"
      ],
      "metadata": {
        "id": "nG-tgj_wXY15"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loss_tri,test_loss_tri"
      ],
      "metadata": {
        "id": "yk4iAhrSbC7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ea7d39-ea8b-49d7-b3b6-d2598d9d6fb9"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(2.2442), tensor(2.2375))"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My trigram model obviously performs better than bigram. The reason i might be having lower test and validation loss is because i have removed regularisation(general practice) during evaluation phase. During training, the regularisation is inflating the loss."
      ],
      "metadata": {
        "id": "qEzk-9wYfGRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "KY5GXxX4foJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 3: Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tyPOhnw2fqHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: I should have had a function which i could have run many times but now in this setting, i will have to write above could again and again**"
      ],
      "metadata": {
        "id": "Pmv4DrWRgnn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a function now"
      ],
      "metadata": {
        "id": "cgUg4F0qi02c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_trigram():\n",
        "  xs, ys = [], []\n",
        "  for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2,ch3 in zip(chs, chs[1:],chs[2:]):\n",
        "      ix1 = stoi[ch1]\n",
        "      ix2 = stoi[ch2]\n",
        "      ix3 = stoi[ch3]\n",
        "      xs.append([ix1,ix2])\n",
        "      ys.append(ix3)\n",
        "  xs = torch.tensor(xs)\n",
        "  ys = torch.tensor(ys)\n",
        "  return xs,ys"
      ],
      "metadata": {
        "id": "1ao55j8OZKSo"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tensor,y_tensor=dataset_trigram()"
      ],
      "metadata": {
        "id": "mt_3pwafjsa8"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs_trainval,xs_test,ys_trainval,ys_test=train_test_split(x_tensor,y_tensor,random_state=42,test_size=0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "MBabICiSjgAo"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs_train,xs_val,ys_train,ys_val=train_test_split(xs_trainval,ys_trainval,random_state=42,test_size=0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "zHDNozu_j90A"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_trigram(xs_train,ys_train):\n",
        "    # initialize the 'network'\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
        "\n",
        "    # gradient descent\n",
        "  for k in range(300):\n",
        "    \n",
        "    # forward pass\n",
        "    xenc = F.one_hot(xs_train, num_classes=27).float() # input to the network: one-hot encoding\n",
        "    xenc=xenc.reshape(xs_train.shape[0],-1)\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(ys_train.shape[0]), ys_train].log().mean() + (1e-15)*(W**2).mean()\n",
        "    print(loss.item())\n",
        "    \n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "    \n",
        "    # update\n",
        "    W.data += -50 * W.grad\n",
        "\n",
        "  return loss.item(),W\n",
        "    "
      ],
      "metadata": {
        "id": "7fvl6cATkWZh"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l,W_train=train_trigram(xs_train,ys_train)"
      ],
      "metadata": {
        "id": "haLMZefWk-cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa1c47c-51e0-4e66-a9f3-e0af6eeb18e1"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.184372901916504\n",
            "3.3568389415740967\n",
            "3.042738914489746\n",
            "2.8721823692321777\n",
            "2.7678630352020264\n",
            "2.69545841217041\n",
            "2.640033006668091\n",
            "2.59609055519104\n",
            "2.560276746749878\n",
            "2.5306482315063477\n",
            "2.505793809890747\n",
            "2.4847426414489746\n",
            "2.4667177200317383\n",
            "2.4511356353759766\n",
            "2.4375243186950684\n",
            "2.4255244731903076\n",
            "2.4148523807525635\n",
            "2.4052915573120117\n",
            "2.39667010307312\n",
            "2.3888540267944336\n",
            "2.3817331790924072\n",
            "2.3752191066741943\n",
            "2.3692378997802734\n",
            "2.36372709274292\n",
            "2.3586337566375732\n",
            "2.353912830352783\n",
            "2.3495264053344727\n",
            "2.345440149307251\n",
            "2.341625213623047\n",
            "2.338055372238159\n",
            "2.334709882736206\n",
            "2.3315675258636475\n",
            "2.328611373901367\n",
            "2.3258259296417236\n",
            "2.323197603225708\n",
            "2.320713758468628\n",
            "2.3183629512786865\n",
            "2.316135883331299\n",
            "2.3140227794647217\n",
            "2.312016010284424\n",
            "2.310107707977295\n",
            "2.308290958404541\n",
            "2.3065595626831055\n",
            "2.304908037185669\n",
            "2.303331136703491\n",
            "2.3018240928649902\n",
            "2.300382137298584\n",
            "2.299001455307007\n",
            "2.2976787090301514\n",
            "2.2964096069335938\n",
            "2.295191526412964\n",
            "2.2940211296081543\n",
            "2.292895793914795\n",
            "2.2918131351470947\n",
            "2.2907705307006836\n",
            "2.2897660732269287\n",
            "2.288797378540039\n",
            "2.287863254547119\n",
            "2.2869603633880615\n",
            "2.286088705062866\n",
            "2.2852463722229004\n",
            "2.284430980682373\n",
            "2.283641815185547\n",
            "2.2828776836395264\n",
            "2.282137632369995\n",
            "2.2814199924468994\n",
            "2.280724048614502\n",
            "2.2800488471984863\n",
            "2.279392957687378\n",
            "2.2787561416625977\n",
            "2.27813720703125\n",
            "2.2775354385375977\n",
            "2.2769505977630615\n",
            "2.276381015777588\n",
            "2.275826930999756\n",
            "2.27528715133667\n",
            "2.27476167678833\n",
            "2.27424955368042\n",
            "2.2737503051757812\n",
            "2.273263454437256\n",
            "2.2727885246276855\n",
            "2.272325038909912\n",
            "2.2718729972839355\n",
            "2.2714314460754395\n",
            "2.2709999084472656\n",
            "2.270578384399414\n",
            "2.270167112350464\n",
            "2.2697641849517822\n",
            "2.2693707942962646\n",
            "2.2689859867095947\n",
            "2.268609046936035\n",
            "2.2682409286499023\n",
            "2.267880439758301\n",
            "2.2675273418426514\n",
            "2.267181634902954\n",
            "2.26684308052063\n",
            "2.2665116786956787\n",
            "2.266186475753784\n",
            "2.2658679485321045\n",
            "2.2655558586120605\n",
            "2.2652499675750732\n",
            "2.2649497985839844\n",
            "2.264655590057373\n",
            "2.264366626739502\n",
            "2.2640833854675293\n",
            "2.263805389404297\n",
            "2.263532876968384\n",
            "2.2632648944854736\n",
            "2.2630019187927246\n",
            "2.2627439498901367\n",
            "2.2624902725219727\n",
            "2.2622411251068115\n",
            "2.2619965076446533\n",
            "2.261756420135498\n",
            "2.2615199089050293\n",
            "2.2612876892089844\n",
            "2.2610597610473633\n",
            "2.2608354091644287\n",
            "2.2606146335601807\n",
            "2.2603979110717773\n",
            "2.2601847648620605\n",
            "2.259974718093872\n",
            "2.2597687244415283\n",
            "2.259565591812134\n",
            "2.259366035461426\n",
            "2.259169340133667\n",
            "2.2589757442474365\n",
            "2.2587857246398926\n",
            "2.2585983276367188\n",
            "2.258413553237915\n",
            "2.2582321166992188\n",
            "2.2580533027648926\n",
            "2.2578773498535156\n",
            "2.2577035427093506\n",
            "2.257533073425293\n",
            "2.2573647499084473\n",
            "2.2571990489959717\n",
            "2.257035732269287\n",
            "2.2568745613098145\n",
            "2.256716251373291\n",
            "2.2565598487854004\n",
            "2.256405830383301\n",
            "2.256254196166992\n",
            "2.2561044692993164\n",
            "2.2559568881988525\n",
            "2.2558114528656006\n",
            "2.2556679248809814\n",
            "2.2555267810821533\n",
            "2.2553870677948\n",
            "2.255249500274658\n",
            "2.2551138401031494\n",
            "2.2549800872802734\n",
            "2.2548482418060303\n",
            "2.2547178268432617\n",
            "2.254589319229126\n",
            "2.254462480545044\n",
            "2.2543373107910156\n",
            "2.25421404838562\n",
            "2.254092216491699\n",
            "2.253971815109253\n",
            "2.2538530826568604\n",
            "2.2537357807159424\n",
            "2.253620147705078\n",
            "2.2535059452056885\n",
            "2.2533931732177734\n",
            "2.253282070159912\n",
            "2.253171920776367\n",
            "2.2530629634857178\n",
            "2.252955913543701\n",
            "2.25285005569458\n",
            "2.2527451515197754\n",
            "2.2526421546936035\n",
            "2.25253963470459\n",
            "2.25243878364563\n",
            "2.2523391246795654\n",
            "2.2522404193878174\n",
            "2.252143383026123\n",
            "2.252046823501587\n",
            "2.2519519329071045\n",
            "2.2518579959869385\n",
            "2.251765489578247\n",
            "2.2516732215881348\n",
            "2.251582384109497\n",
            "2.251492738723755\n",
            "2.251404285430908\n",
            "2.251316547393799\n",
            "2.251229763031006\n",
            "2.25114369392395\n",
            "2.2510592937469482\n",
            "2.2509753704071045\n",
            "2.2508926391601562\n",
            "2.2508106231689453\n",
            "2.2507293224334717\n",
            "2.2506489753723145\n",
            "2.2505698204040527\n",
            "2.250491142272949\n",
            "2.250413656234741\n",
            "2.2503368854522705\n",
            "2.250260591506958\n",
            "2.25018572807312\n",
            "2.2501108646392822\n",
            "2.250037670135498\n",
            "2.249964714050293\n",
            "2.249892473220825\n",
            "2.2498209476470947\n",
            "2.2497506141662598\n",
            "2.249680757522583\n",
            "2.2496116161346436\n",
            "2.2495431900024414\n",
            "2.2494752407073975\n",
            "2.249408483505249\n",
            "2.2493417263031006\n",
            "2.2492761611938477\n",
            "2.249210834503174\n",
            "2.2491466999053955\n",
            "2.2490828037261963\n",
            "2.2490196228027344\n",
            "2.2489569187164307\n",
            "2.248894691467285\n",
            "2.248833656311035\n",
            "2.2487728595733643\n",
            "2.2487127780914307\n",
            "2.248652935028076\n",
            "2.248594284057617\n",
            "2.248535633087158\n",
            "2.2484779357910156\n",
            "2.248420238494873\n",
            "2.248363733291626\n",
            "2.248307228088379\n",
            "2.2482516765594482\n",
            "2.2481961250305176\n",
            "2.2481420040130615\n",
            "2.2480874061584473\n",
            "2.2480337619781494\n",
            "2.2479805946350098\n",
            "2.2479279041290283\n",
            "2.247875452041626\n",
            "2.247823715209961\n",
            "2.247772693634033\n",
            "2.2477219104766846\n",
            "2.247671604156494\n",
            "2.2476212978363037\n",
            "2.2475719451904297\n",
            "2.247523069381714\n",
            "2.247474431991577\n",
            "2.2474260330200195\n",
            "2.247378349304199\n",
            "2.247330904006958\n",
            "2.247284412384033\n",
            "2.2472376823425293\n",
            "2.2471916675567627\n",
            "2.2471463680267334\n",
            "2.247100830078125\n",
            "2.247055768966675\n",
            "2.247011423110962\n",
            "2.246967315673828\n",
            "2.2469234466552734\n",
            "2.246880054473877\n",
            "2.2468371391296387\n",
            "2.2467944622039795\n",
            "2.2467522621154785\n",
            "2.2467103004455566\n",
            "2.246669054031372\n",
            "2.2466275691986084\n",
            "2.246586561203003\n",
            "2.2465462684631348\n",
            "2.2465062141418457\n",
            "2.2464659214019775\n",
            "2.246426582336426\n",
            "2.246387481689453\n",
            "2.2463483810424805\n",
            "2.246309757232666\n",
            "2.2462716102600098\n",
            "2.2462337017059326\n",
            "2.2461960315704346\n",
            "2.2461588382720947\n",
            "2.246121644973755\n",
            "2.2460849285125732\n",
            "2.24604868888855\n",
            "2.2460124492645264\n",
            "2.245976686477661\n",
            "2.245940923690796\n",
            "2.245905637741089\n",
            "2.24587082862854\n",
            "2.2458362579345703\n",
            "2.2458014488220215\n",
            "2.245767116546631\n",
            "2.2457334995269775\n",
            "2.245699644088745\n",
            "2.245666027069092\n",
            "2.245633363723755\n",
            "2.2456002235412598\n",
            "2.245567798614502\n",
            "2.245535373687744\n",
            "2.2455031871795654\n",
            "2.245471239089966\n",
            "2.2454397678375244\n",
            "2.245408296585083\n",
            "2.2453773021698\n",
            "2.2453463077545166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l"
      ],
      "metadata": {
        "id": "jbwHhxmnlJPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb7e7d7-a9dd-47d5-b6eb-b7154cca820b"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2453463077545166"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(W_train,xs_val,ys_val)"
      ],
      "metadata": {
        "id": "3uZzOb3SmJZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b90ca12-328b-4d08-b53d-2b66956c42f5"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2429)"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(W_train,xs_test,ys_test)"
      ],
      "metadata": {
        "id": "3zKckJsspxg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c7c6c6-f6e7-40ba-d0dd-ef52b0f8a9b2"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2360)"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found that the regularisation parameter had to be decreased quite a lot to have comparable results in training,validation. The test loss is lesser than train and validation losses"
      ],
      "metadata": {
        "id": "kVStDUMXs7Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****"
      ],
      "metadata": {
        "id": "vRxrinEazQ59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercises 4: E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
      ],
      "metadata": {
        "id": "sD4pKlKQv6MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W.shape"
      ],
      "metadata": {
        "id": "a-IPpacFJoxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1bc2040-3aae-4f3d-a1cd-991a8f72d1b7"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([54, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.one_hot(xs_train[:1],num_classes=27)"
      ],
      "metadata": {
        "id": "OE1Seg2BKrPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d1967f-8f73-44e8-b209-dbf4dc1abd86"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs_train[:2],ys_train[:2]"
      ],
      "metadata": {
        "id": "dQXlKTvos5RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b91ffc2a-6506-4cbc-af90-57c6f6e7a5a4"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[12,  5],\n",
              "         [ 0, 20]]), tensor([14,  1]))"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will be the activation \n",
        "activation_for_second_across_27_neurons=W[0]+W[20]"
      ],
      "metadata": {
        "id": "wslycxxvO6Yx"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h=F.one_hot(xs_train[:2],num_classes=27)"
      ],
      "metadata": {
        "id": "JgJzBD_hQhtW"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h"
      ],
      "metadata": {
        "id": "uR4Lo6TzyRlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd15a340-834b-4886-9020-44ed1cebd777"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0]],\n",
              "\n",
              "        [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "          0, 0, 0, 0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h.reshape(2,-1)"
      ],
      "metadata": {
        "id": "SFfq8dUIyGS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97efd8c-8551-4440-9927-ca6c1bcc05b8"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "         0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(h.reshape(2,-1).float()@W)"
      ],
      "metadata": {
        "id": "gpXgAsjExuOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9068ef9c-0d69-4901-9852-0c70b05f03e8"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.8816,  1.1855, -1.2450, -0.3418, -0.2719,  2.1016, -1.3596, -1.2842,\n",
              "         -0.2921,  1.7929, -1.3506, -0.2122,  1.1643,  0.5749,  2.9728,  0.1881,\n",
              "         -1.0421, -2.5379,  1.5135,  1.3063,  0.6913, -1.2894,  0.5245, -0.5295,\n",
              "          0.0726,  2.0374, -0.1776],\n",
              "        [-2.4900,  4.3650, -1.1868, -0.5876, -0.7969,  3.4873, -1.6074, -2.4537,\n",
              "          3.3500,  3.0842, -2.1027, -1.8474,  1.7279, -0.4016, -1.0850,  3.8921,\n",
              "         -2.3495, -2.0927,  3.1016,  0.0634,  2.0082,  2.1951,  0.1874, -0.3798,\n",
              "         -2.0396,  2.7364,  1.9588]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W[12]"
      ],
      "metadata": {
        "id": "1aehTrhwxxfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3944c795-9834-4356-f624-14e75667ec14"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.5924,  1.6930, -0.8967,  0.1349, -0.7252,  1.8565, -1.0747, -0.7751,\n",
              "         0.4439,  1.9663, -0.4620, -0.0523, -1.0233, -0.4285,  0.8220,  1.0321,\n",
              "        -0.2697, -0.7843,  0.1371,  0.2453, -0.1357,  0.5407, -0.6223,  1.0016,\n",
              "         0.5405,  1.4884, -0.0439], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W[27+5]"
      ],
      "metadata": {
        "id": "dIZgYaqLy9pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9adfd1ac-fa69-4162-bf5b-5da954ca86ff"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.2892, -0.5075, -0.3483, -0.4767,  0.4534,  0.2451, -0.2848, -0.5091,\n",
              "        -0.7360, -0.1735, -0.8886, -0.1599,  2.1876,  1.0034,  2.1508, -0.8440,\n",
              "        -0.7724, -1.7536,  1.3764,  1.0610,  0.8269, -1.8301,  1.1468, -1.5311,\n",
              "        -0.4679,  0.5490, -0.1337], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "li=[]\n",
        "li.append((W[12]+W[27+5].view(1,27)))\n",
        "li.append((W[0]+W[27+20]).view(1,27))\n"
      ],
      "metadata": {
        "id": "5qZ8XtVZzp3j"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "li"
      ],
      "metadata": {
        "id": "qlYTignQ8o0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b34351e2-8eca-4600-afe6-c2212d3c15e8"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 2.8816,  1.1855, -1.2450, -0.3418, -0.2719,  2.1016, -1.3596, -1.2842,\n",
              "          -0.2921,  1.7929, -1.3506, -0.2122,  1.1643,  0.5749,  2.9728,  0.1881,\n",
              "          -1.0421, -2.5379,  1.5135,  1.3063,  0.6913, -1.2894,  0.5245, -0.5295,\n",
              "           0.0726,  2.0374, -0.1776]], grad_fn=<AddBackward0>),\n",
              " tensor([[-2.4900,  4.3650, -1.1868, -0.5876, -0.7969,  3.4873, -1.6074, -2.4537,\n",
              "           3.3500,  3.0842, -2.1027, -1.8474,  1.7279, -0.4016, -1.0850,  3.8921,\n",
              "          -2.3495, -2.0927,  3.1016,  0.0634,  2.0082,  2.1951,  0.1874, -0.3798,\n",
              "          -2.0396,  2.7364,  1.9588]], grad_fn=<ViewBackward0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat(li).shape"
      ],
      "metadata": {
        "id": "DY8nlelL77ML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62ddbba-be50-4a80-e545-e821e6fc5b07"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_trigram():\n",
        "  xs, ys = [], []\n",
        "  for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2,ch3 in zip(chs, chs[1:],chs[2:]):\n",
        "      ix1 = stoi[ch1]\n",
        "      ix2 = stoi[ch2]\n",
        "      ix3 = stoi[ch3]\n",
        "      xs.append([ix1,ix2])\n",
        "      ys.append(ix3)\n",
        "  xs = torch.tensor(xs)\n",
        "  ys = torch.tensor(ys)\n",
        "  return xs,ys"
      ],
      "metadata": {
        "id": "SRpoy0UY1vpR"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tensor,y_tensor=dataset_trigram()"
      ],
      "metadata": {
        "id": "yuKw2zlu4a2R"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs_trainval,xs_test,ys_trainval,ys_test=train_test_split(x_tensor,y_tensor,random_state=42,test_size=0.1,shuffle=True)"
      ],
      "metadata": {
        "id": "r7RMT3c64nA8"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_trigram(xs_train,ys_train):\n",
        "    # initialize the 'network'\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
        "\n",
        "    # gradient descent\n",
        "  for k in range(300):\n",
        "    \n",
        "    # forward pass\n",
        "    li=[]\n",
        "    for p in xs_train:\n",
        "      li.append(W[p[0].item()]+W[27+p[1].item()].view(1,27))\n",
        "    logits=torch.cat(li)\n",
        "    # xenc = F.one_hot(xs_train, num_classes=27).float() # input to the network: one-hot encoding\n",
        "    #xenc=xenc.reshape(xs_train.shape[0],-1)\n",
        "    #logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(ys_train.shape[0]), ys_train].log().mean() + (1e-15)*(W**2).mean()\n",
        "    print(loss.item())\n",
        "    \n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "    \n",
        "    # update\n",
        "    W.data += -50 * W.grad\n",
        "\n",
        "  return loss.item(),W"
      ],
      "metadata": {
        "id": "OWBAbkSV4osv"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l,W_train=train_trigram(xs_train,ys_train)"
      ],
      "metadata": {
        "id": "IgBjaVjh4zKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba367c9b-cb5a-400c-f8c8-fc1a58de58e1"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.184372901916504\n",
            "3.356835126876831\n",
            "3.042736530303955\n",
            "2.8721816539764404\n",
            "2.767862558364868\n",
            "2.695457935333252\n",
            "2.6400327682495117\n",
            "2.596090316772461\n",
            "2.560276746749878\n",
            "2.5306482315063477\n",
            "2.505793809890747\n",
            "2.4847426414489746\n",
            "2.4667177200317383\n",
            "2.4511356353759766\n",
            "2.4375240802764893\n",
            "2.4255247116088867\n",
            "2.4148523807525635\n",
            "2.4052915573120117\n",
            "2.39667010307312\n",
            "2.3888537883758545\n",
            "2.3817331790924072\n",
            "2.3752191066741943\n",
            "2.3692378997802734\n",
            "2.36372709274292\n",
            "2.3586337566375732\n",
            "2.353912830352783\n",
            "2.3495264053344727\n",
            "2.345440149307251\n",
            "2.3416249752044678\n",
            "2.338055372238159\n",
            "2.334709882736206\n",
            "2.3315672874450684\n",
            "2.328611373901367\n",
            "2.3258261680603027\n",
            "2.323197603225708\n",
            "2.320713758468628\n",
            "2.3183629512786865\n",
            "2.316135883331299\n",
            "2.314023017883301\n",
            "2.3120157718658447\n",
            "2.310107469558716\n",
            "2.308290958404541\n",
            "2.3065595626831055\n",
            "2.304908037185669\n",
            "2.303330898284912\n",
            "2.3018240928649902\n",
            "2.300382137298584\n",
            "2.299001455307007\n",
            "2.2976787090301514\n",
            "2.2964096069335938\n",
            "2.295191526412964\n",
            "2.2940211296081543\n",
            "2.292895555496216\n",
            "2.2918131351470947\n",
            "2.2907705307006836\n",
            "2.2897660732269287\n",
            "2.288797616958618\n",
            "2.28786301612854\n",
            "2.2869603633880615\n",
            "2.286088705062866\n",
            "2.2852461338043213\n",
            "2.284430980682373\n",
            "2.283641815185547\n",
            "2.2828776836395264\n",
            "2.282137393951416\n",
            "2.2814199924468994\n",
            "2.280724048614502\n",
            "2.2800486087799072\n",
            "2.279392957687378\n",
            "2.2787561416625977\n",
            "2.278137445449829\n",
            "2.2775354385375977\n",
            "2.2769505977630615\n",
            "2.276381015777588\n",
            "2.275826930999756\n",
            "2.27528715133667\n",
            "2.27476167678833\n",
            "2.27424955368042\n",
            "2.2737503051757812\n",
            "2.273263454437256\n",
            "2.2727885246276855\n",
            "2.272325038909912\n",
            "2.2718727588653564\n",
            "2.2714314460754395\n",
            "2.2709999084472656\n",
            "2.270578384399414\n",
            "2.2701668739318848\n",
            "2.2697644233703613\n",
            "2.2693707942962646\n",
            "2.2689859867095947\n",
            "2.268609046936035\n",
            "2.2682409286499023\n",
            "2.267880439758301\n",
            "2.2675273418426514\n",
            "2.267181396484375\n",
            "2.26684308052063\n",
            "2.2665116786956787\n",
            "2.266186475753784\n",
            "2.2658679485321045\n",
            "2.2655560970306396\n",
            "2.2652499675750732\n",
            "2.2649495601654053\n",
            "2.264655590057373\n",
            "2.264366626739502\n",
            "2.2640833854675293\n",
            "2.263805389404297\n",
            "2.263532876968384\n",
            "2.2632648944854736\n",
            "2.2630019187927246\n",
            "2.2627439498901367\n",
            "2.2624902725219727\n",
            "2.2622408866882324\n",
            "2.2619965076446533\n",
            "2.261756420135498\n",
            "2.2615199089050293\n",
            "2.2612876892089844\n",
            "2.261059522628784\n",
            "2.2608354091644287\n",
            "2.2606146335601807\n",
            "2.2603979110717773\n",
            "2.2601847648620605\n",
            "2.259974718093872\n",
            "2.2597687244415283\n",
            "2.259565591812134\n",
            "2.259366035461426\n",
            "2.259169340133667\n",
            "2.2589757442474365\n",
            "2.2587857246398926\n",
            "2.2585980892181396\n",
            "2.258413553237915\n",
            "2.2582321166992188\n",
            "2.2580533027648926\n",
            "2.2578771114349365\n",
            "2.2577035427093506\n",
            "2.257532835006714\n",
            "2.2573647499084473\n",
            "2.2571990489959717\n",
            "2.257035732269287\n",
            "2.2568745613098145\n",
            "2.256716251373291\n",
            "2.2565598487854004\n",
            "2.256405830383301\n",
            "2.256254196166992\n",
            "2.2561044692993164\n",
            "2.2559568881988525\n",
            "2.2558114528656006\n",
            "2.2556679248809814\n",
            "2.2555267810821533\n",
            "2.255387306213379\n",
            "2.255249500274658\n",
            "2.2551143169403076\n",
            "2.2549800872802734\n",
            "2.2548482418060303\n",
            "2.2547178268432617\n",
            "2.254589557647705\n",
            "2.254462718963623\n",
            "2.2543373107910156\n",
            "2.25421404838562\n",
            "2.254092216491699\n",
            "2.253971815109253\n",
            "2.2538530826568604\n",
            "2.2537357807159424\n",
            "2.253620147705078\n",
            "2.2535059452056885\n",
            "2.2533929347991943\n",
            "2.253282070159912\n",
            "2.253171920776367\n",
            "2.2530629634857178\n",
            "2.252955913543701\n",
            "2.25285005569458\n",
            "2.2527451515197754\n",
            "2.2526419162750244\n",
            "2.25253963470459\n",
            "2.25243878364563\n",
            "2.2523391246795654\n",
            "2.2522404193878174\n",
            "2.252143383026123\n",
            "2.252046823501587\n",
            "2.2519519329071045\n",
            "2.2518579959869385\n",
            "2.251765489578247\n",
            "2.2516732215881348\n",
            "2.251582384109497\n",
            "2.251492738723755\n",
            "2.251404285430908\n",
            "2.251316547393799\n",
            "2.2512295246124268\n",
            "2.2511441707611084\n",
            "2.2510592937469482\n",
            "2.2509753704071045\n",
            "2.2508926391601562\n",
            "2.2508106231689453\n",
            "2.250729560852051\n",
            "2.2506489753723145\n",
            "2.2505698204040527\n",
            "2.250491142272949\n",
            "2.250413656234741\n",
            "2.2503368854522705\n",
            "2.250260591506958\n",
            "2.25018572807312\n",
            "2.2501113414764404\n",
            "2.250037670135498\n",
            "2.249964475631714\n",
            "2.249892473220825\n",
            "2.2498209476470947\n",
            "2.2497506141662598\n",
            "2.249680757522583\n",
            "2.2496116161346436\n",
            "2.2495431900024414\n",
            "2.2494752407073975\n",
            "2.249408006668091\n",
            "2.2493417263031006\n",
            "2.2492761611938477\n",
            "2.249210834503174\n",
            "2.2491466999053955\n",
            "2.2490828037261963\n",
            "2.2490196228027344\n",
            "2.2489569187164307\n",
            "2.248894691467285\n",
            "2.248833656311035\n",
            "2.2487728595733643\n",
            "2.2487127780914307\n",
            "2.248652935028076\n",
            "2.248594284057617\n",
            "2.248535633087158\n",
            "2.2484779357910156\n",
            "2.248420238494873\n",
            "2.248363733291626\n",
            "2.248307228088379\n",
            "2.248251438140869\n",
            "2.2481961250305176\n",
            "2.2481417655944824\n",
            "2.2480874061584473\n",
            "2.2480337619781494\n",
            "2.2479805946350098\n",
            "2.2479281425476074\n",
            "2.247875690460205\n",
            "2.247823715209961\n",
            "2.247772693634033\n",
            "2.2477216720581055\n",
            "2.247671604156494\n",
            "2.2476212978363037\n",
            "2.2475719451904297\n",
            "2.247523069381714\n",
            "2.247474193572998\n",
            "2.2474260330200195\n",
            "2.247378349304199\n",
            "2.247331142425537\n",
            "2.247284412384033\n",
            "2.2472376823425293\n",
            "2.247191905975342\n",
            "2.2471461296081543\n",
            "2.247100830078125\n",
            "2.247055768966675\n",
            "2.247011423110962\n",
            "2.2469675540924072\n",
            "2.2469234466552734\n",
            "2.246880054473877\n",
            "2.2468371391296387\n",
            "2.2467944622039795\n",
            "2.2467522621154785\n",
            "2.2467103004455566\n",
            "2.246669054031372\n",
            "2.2466278076171875\n",
            "2.246586561203003\n",
            "2.2465462684631348\n",
            "2.2465062141418457\n",
            "2.2464659214019775\n",
            "2.246426582336426\n",
            "2.246387481689453\n",
            "2.2463483810424805\n",
            "2.246309757232666\n",
            "2.2462716102600098\n",
            "2.2462337017059326\n",
            "2.2461960315704346\n",
            "2.2461588382720947\n",
            "2.246121644973755\n",
            "2.2460851669311523\n",
            "2.24604868888855\n",
            "2.2460124492645264\n",
            "2.245976686477661\n",
            "2.245941162109375\n",
            "2.245905637741089\n",
            "2.24587082862854\n",
            "2.2458362579345703\n",
            "2.2458014488220215\n",
            "2.245767593383789\n",
            "2.2457334995269775\n",
            "2.245699644088745\n",
            "2.245666027069092\n",
            "2.245633363723755\n",
            "2.2456002235412598\n",
            "2.245567798614502\n",
            "2.245535373687744\n",
            "2.2455031871795654\n",
            "2.245471239089966\n",
            "2.2454397678375244\n",
            "2.245408296585083\n",
            "2.2453773021698\n",
            "2.2453463077545166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This above method performs same as what we did earlier"
      ],
      "metadata": {
        "id": "hOBQ3-2HTuq8"
      }
    }
  ]
}